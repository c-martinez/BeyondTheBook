{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import BeautifulSoup\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mwclient\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import btb.utils.tools as btbtools\n",
    "import btb.utils.wikiquery as wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiNL = mwclient.Site('nl.wikipedia.org')\n",
    "wikiEN = mwclient.Site('en.wikipedia.org')\n",
    "bots = wq.getAllBots(wikiEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__NAME_CACHE = {}\n",
    "__INTEREST_CACHE = {}\n",
    "\n",
    "def getFromGlobalNameCache(name):\n",
    "    try:\n",
    "        return __NAME_CACHE[name]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def saveToGlobalNameCache(name, value):\n",
    "    __NAME_CACHE[name] = value\n",
    "\n",
    "def getFromGlobalInterestCache(name):\n",
    "    try:\n",
    "        return __INTEREST_CACHE[name]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def saveToGlobalInterestCache(name, value):\n",
    "    __INTEREST_CACHE[name] = value\n",
    "\n",
    "def syncCaches():\n",
    "    global __NAME_CACHE\n",
    "    global __INTEREST_CACHE\n",
    "    try:\n",
    "        nameCacheFile = pickle.load(open('NAME_CACHE.pkl','r'))\n",
    "        __NAME_CACHE = dict(__NAME_CACHE.items() + nameCacheFile.items())\n",
    "        \n",
    "        sizePre = len(nameCacheFile)\n",
    "        sizePost = len(__NAME_CACHE)\n",
    "        if sizePre!=sizePost:\n",
    "            print 'Cache grew by: ',(sizePost - sizePre)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        interestCacheFile = pickle.load(open('INTEREST_CACHE.pkl','r'))\n",
    "        __INTEREST_CACHE = dict(__INTEREST_CACHE.items() + interestCacheFile.items())\n",
    "        \n",
    "        sizePre = len(interestCacheFile)\n",
    "        sizePost = len(__INTEREST_CACHE)\n",
    "        if sizePre!=sizePost:\n",
    "            print 'Cache 2 grew by: ',(sizePost - sizePre)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pickle.dump(__NAME_CACHE, open('NAME_CACHE.pkl', 'w'))\n",
    "    pickle.dump(__INTEREST_CACHE, open('INTEREST_CACHE.pkl', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getName(nym_xml):\n",
    "    forms = nym_xml.findAll('form', {'type': 'nym'})\n",
    "    if len(forms)==1:\n",
    "        return forms[0].getText()\n",
    "    else:\n",
    "        print '!!! getName(nym_xml) -- length = ',len(forms)\n",
    "        return None\n",
    "\n",
    "def getNymData(nym_xml):\n",
    "    '''\n",
    "    Returns:\n",
    "      freq    The frequency of term on any form it was observed\n",
    "      name    The normalized way in which the text has been observed\n",
    "      nstype  Namescape type of entity: person, location, etc.\n",
    "    '''\n",
    "    freqs = nym_xml.findAll('usg', { 'type': 'frequency' })\n",
    "    freq = np.array([ int(f.getText()) for f in freqs]).max()\n",
    "    name = getName(nym_xml)\n",
    "    nstype = nym_xml.get('ns:type')\n",
    "    \n",
    "    return freq, name, nstype\n",
    "\n",
    "def getLangTitle(sourceWiki, title, targetLang='en'):\n",
    "    page = sourceWiki.Pages[title]\n",
    "    page = page.resolve_redirect()\n",
    "\n",
    "    for lang,langTitle in page.langlinks():\n",
    "        if lang==targetLang:\n",
    "            return langTitle\n",
    "    return None\n",
    "\n",
    "def getAllTitleOptions(title):\n",
    "    words = title.split(' ')\n",
    "    cases = range(2) # [ 0 1 ] ==> 0 lower, 1 title case\n",
    "    nWords = len(words)\n",
    "    titleOpts = []\n",
    "    for x in itertools.product(cases, repeat=nWords):\n",
    "        title =  [ words[i].lower() if x[i]==0 else words[i].title() for i in range(nWords) ]\n",
    "        titleOpts.append(' '.join(title))\n",
    "    return titleOpts\n",
    "\n",
    "def findFirstTitleMatch(sourceWiki, origTitle, targetLang='en', debug=False):\n",
    "    cached = getFromGlobalNameCache(origTitle)\n",
    "#    if cached is not None:\n",
    "#        print 'Cached saved time!'\n",
    "#        return cached\n",
    "    \n",
    "    titles = getAllTitleOptions(origTitle)\n",
    "    for title in titles:\n",
    "        title = mwclient.page.Page.normalize_title(title)\n",
    "        candidate = getLangTitle(sourceWiki, title, targetLang=targetLang)\n",
    "        \n",
    "        if debug:\n",
    "            print title,'-->',candidate\n",
    "        \n",
    "        if candidate is not None:\n",
    "            saveToGlobalNameCache(origTitle, candidate)\n",
    "            return candidate\n",
    "    return None # No translation was found !\n",
    "\n",
    "def wikiCountryInterest(wiki, pageTitle):\n",
    "    cached = getFromGlobalInterestCache(pageTitle)\n",
    "#    if cached is not None:\n",
    "#        print 'Cached saved time 2!'\n",
    "#        return cached\n",
    "    try:\n",
    "        ips, usrs, nrevs = wq.getContributionsForPage(wiki, pageTitle)\n",
    "        knwRevs, conf, nIP, nUsr, nBot, nUnkn = btbtools.prepareData(ips, usrs, bots)\n",
    "        expEdits = wq.getTotalContributions()\n",
    "\n",
    "        cmpEdits = btbtools.compareEdits(expEdits, knwRevs)\n",
    "\n",
    "        nl_e, nl_o, nl_m = cmpEdits['NL']\n",
    "        ca_e, ca_o, ca_m = cmpEdits['CA']\n",
    "\n",
    "        saveToGlobalInterestCache(pageTitle, (nl_o, ca_o, conf))\n",
    "        return (nl_o, ca_o, conf)\n",
    "    except Exception as err:\n",
    "        print 'PATCH IT! Error for word \"'+pageTitle+'\":',err.__class__,' ->',err\n",
    "        return None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# karinaFile = '../data/corpus.karina.nerINL.2012-10-26/nl.ns.d.9789020417159.k.xml'\n",
    "karinaFile = '../data/corpus.karina.nerINL.2012-10-26/nl.ns.d.9789044301717.k.xml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tei = open(karinaFile).read()\n",
    "teiSoup = BeautifulSoup.BeautifulSoup(tei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nymList = teiSoup.findAll('nym')\n",
    "nymData = [getNymData(nym) for nym in nymList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataEn = []\n",
    "\n",
    "for freq,name,nstype in nymData:\n",
    "    try:\n",
    "        match = findFirstTitleMatch(wikiNL, name, targetLang='en')\n",
    "        if match is not None:\n",
    "            dataEn.append((name,match,nstype,freq))\n",
    "    except Exception as err:\n",
    "        print 'Error for word \"'+name+'\":',err.__class__,' ->',err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dataEn, columns=['OrigWord','Word','Type','Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['TEMP'] = data['Word'].apply(lambda x: wikiCountryInterest(wikiEN, x))\n",
    "data['NLO'] = data['TEMP'].apply(lambda x: x[0])\n",
    "data['CAO'] = data['TEMP'].apply(lambda x: x[1])\n",
    "data['Conf'] = data['TEMP'].apply(lambda x: x[2])\n",
    "del data['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expEdits = wq.getTotalContributions()\n",
    "NLE = expEdits['NL']\n",
    "CAE = expEdits['CA']\n",
    "\n",
    "data['Interest-NL'] = data.apply(lambda x: btbtools.relativeInterest(NLE, x['NLO']), axis=1)\n",
    "data['Interest-CA'] = data.apply(lambda x: btbtools.relativeInterest(CAE, x['CAO']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nItems = data['Count'].sum()\n",
    "data['Total-NL'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-NL'],axis=1)\n",
    "data['Total-CA'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-CA'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_pickle('data-karina.tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processKarinaFile(inFile_XML, outFile_PKL):\n",
    "    syncCaches()\n",
    "    tei = open(inFile_XML).read()\n",
    "    teiSoup = BeautifulSoup.BeautifulSoup(tei)\n",
    "\n",
    "    nymList = teiSoup.findAll('nym')\n",
    "    nymData = [getNymData(nym) for nym in nymList]\n",
    "\n",
    "    dataEn = []\n",
    "\n",
    "    for freq,name,nstype in nymData:\n",
    "        try:\n",
    "            match = findFirstTitleMatch(wikiNL, name, targetLang='en')\n",
    "            if match is not None:\n",
    "                dataEn.append((name,match,nstype,freq))\n",
    "            else:\n",
    "                print 'Warning: No English data for word: ',name\n",
    "        except Exception as err:\n",
    "            print 'Error for word \"'+name+'\":',err.__class__,' ->',err\n",
    "    syncCaches()\n",
    "        \n",
    "    data = pd.DataFrame(dataEn, columns=['OrigWord','Word','Type','Count'])\n",
    "\n",
    "    data['TEMP'] = data['Word'].apply(lambda x: wikiCountryInterest(wikiEN, x))\n",
    "    data['NLO'] = data['TEMP'].apply(lambda x: x[0])\n",
    "    data['CAO'] = data['TEMP'].apply(lambda x: x[1])\n",
    "    data['Conf'] = data['TEMP'].apply(lambda x: x[2])\n",
    "    del data['TEMP']\n",
    "\n",
    "    expEdits = wq.getTotalContributions()\n",
    "    NLE = expEdits['NL']\n",
    "    CAE = expEdits['CA']\n",
    "\n",
    "    data['Interest-NL'] = data.apply(lambda x: btbtools.relativeInterest(NLE, x['NLO']), axis=1)\n",
    "    data['Interest-CA'] = data.apply(lambda x: btbtools.relativeInterest(CAE, x['CAO']), axis=1)\n",
    "\n",
    "    nItems = data['Count'].sum()\n",
    "    data['Total-NL'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-NL'],axis=1)\n",
    "    data['Total-CA'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-CA'],axis=1)\n",
    "\n",
    "    data.to_pickle(outFile_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inFiles = glob.glob('../data/corpus.karina.nerINL.2012-10-26/*.xml')\n",
    "outFiles = [ inFile.replace('.xml', '.pkl') for inFile in inFiles ]\n",
    "\n",
    "#inFiles = [\n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9022914186.s.xml', \n",
    "##    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021413353.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021413396.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431294.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404319.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9044604279.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9085420415.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9041410252.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029026561.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021412411.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404114.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.902340744x.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9024292646.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029561610.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.902230292X.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431251.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404866.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029505249.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9053333029.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029518219.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9064811091.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023409272.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9057134829.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021485060.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029554606.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9025426158.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029530421.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9074336825.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029098961.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9041409068.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.906291232X.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029528893.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.905000802X.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9027420351.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023432592.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431189.s.xml', \n",
    "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9025802109.s.xml'\n",
    "#]\n",
    "# outFiles = [ inFile.replace('.xml', '.pkl') for inFile in inFiles ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fin, fout in zip(inFiles, outFiles):\n",
    "    if not os.path.isfile(fout):\n",
    "        print 'Processing ',fin,'...'\n",
    "        processKarinaFile(fin, fout)\n",
    "    else:\n",
    "        print 'Skipping ',fin,'...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATCH !\n",
    "First check for files with missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "expEdits = wq.getTotalContributions()\n",
    "NLE = expEdits['NL']\n",
    "CAE = expEdits['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def needsPatching(data):\n",
    "    dataMissing = data[data['NLO'].isnull()]\n",
    "    if len(dataMissing)==0:\n",
    "        return []\n",
    "    else:\n",
    "        return dataMissing['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for pklFile in glob.glob('../data/corpus.karina.nerINL.2012-10-26/*.pkl'):\n",
    "for pklFile in glob.glob('../data/corpus.sanders.nerINL.2012-10-24/*.pkl'):\n",
    "    data = pickle.load(open(pklFile, 'r'))\n",
    "    toPatch = needsPatching(data)\n",
    "    if len(toPatch)>0:\n",
    "        print pklFile, toPatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any, fix those missing values\n",
    " - Load the file with missing value\n",
    " - Compute missing values\n",
    " - Replace computed values on data frame\n",
    " - Save data frame to original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrigWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>NLO</th>\n",
       "      <th>CAO</th>\n",
       "      <th>Conf</th>\n",
       "      <th>Interest-NL</th>\n",
       "      <th>Interest-CA</th>\n",
       "      <th>Total-NL</th>\n",
       "      <th>Total-CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> BOCHEL</td>\n",
       "      <td> Kyphosis</td>\n",
       "      <td> person</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrigWord      Word    Type  Count  NLO  CAO  Conf  Interest-NL  \\\n",
       "16   BOCHEL  Kyphosis  person      1  NaN  NaN   NaN          NaN   \n",
       "\n",
       "    Interest-CA  Total-NL  Total-CA  \n",
       "16          NaN       NaN       NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'Kyphosis'\n",
    "pklFile = '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9057592576.s.pkl'\n",
    "\n",
    "data = pickle.load(open(pklFile, 'r'))\n",
    "data[data['Word']==word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrigWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>NLO</th>\n",
       "      <th>CAO</th>\n",
       "      <th>Conf</th>\n",
       "      <th>Interest-NL</th>\n",
       "      <th>Interest-CA</th>\n",
       "      <th>Total-NL</th>\n",
       "      <th>Total-CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> BOCHEL</td>\n",
       "      <td> Kyphosis</td>\n",
       "      <td> person</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0.016327</td>\n",
       "      <td> 0.065306</td>\n",
       "      <td> 0.528017</td>\n",
       "      <td> 0.51</td>\n",
       "      <td> 0.173125</td>\n",
       "      <td> 0.075431</td>\n",
       "      <td> 0.025606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrigWord      Word    Type  Count       NLO       CAO      Conf  \\\n",
       "16   BOCHEL  Kyphosis  person      1  0.016327  0.065306  0.528017   \n",
       "\n",
       "    Interest-NL  Interest-CA  Total-NL  Total-CA  \n",
       "16         0.51     0.173125  0.075431  0.025606  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlo,cao,conf = wikiCountryInterest(wikiEN, word)\n",
    "\n",
    "idx = data[data['Word']==word].index\n",
    "assert len(idx)==1 # If more than 1 index, something is wrong...\n",
    "idx = idx[0]\n",
    "\n",
    "inl = btbtools.relativeInterest(NLE, nlo)\n",
    "ica = btbtools.relativeInterest(CAE, cao)\n",
    "nItems = data['Count'].sum()\n",
    "count = data['Count'][idx]\n",
    "\n",
    "tnl = 100 * count / nItems * conf * inl\n",
    "tca = 100 * count / nItems * conf * ica\n",
    "\n",
    "data.loc[idx,'NLO'] = nlo\n",
    "data.loc[idx,'CAO'] = cao\n",
    "data.loc[idx,'Conf'] = conf\n",
    "data.loc[idx,'Interest-NL'] = inl\n",
    "data.loc[idx,'Interest-CA'] = ica\n",
    "data.loc[idx,'Total-NL'] = tnl\n",
    "data.loc[idx,'Total-CA'] = tca\n",
    "\n",
    "data[data['Word']==word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(data,open(pklFile, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 2)",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "signature": "sha256:c5c4748e58f07987f517ba2f3865c277feb74ab488576077deedd8b7d0f2284f"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}