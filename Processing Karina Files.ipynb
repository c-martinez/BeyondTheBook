{
 "metadata": {
  "name": "",
  "signature": "sha256:eda2149736b06cc363070a2d82b592b052c4f9bde3ec0898c4f7d6e56fd7766a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "import BeautifulSoup\n",
      "import numpy as np\n",
      "import itertools\n",
      "import mwclient\n",
      "import pandas as pd\n",
      "\n",
      "import pickle\n",
      "\n",
      "import btb.utils.tools as btbtools\n",
      "import btb.utils.wikiquery as wq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wikiNL = mwclient.Site('nl.wikipedia.org')\n",
      "wikiEN = mwclient.Site('en.wikipedia.org')\n",
      "bots = wq.getAllBots(wikiEN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__NAME_CACHE = {}\n",
      "__INTEREST_CACHE = {}\n",
      "\n",
      "def getFromGlobalNameCache(name):\n",
      "    try:\n",
      "        return __NAME_CACHE[name]\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "def saveToGlobalNameCache(name, value):\n",
      "    __NAME_CACHE[name] = value\n",
      "\n",
      "def getFromGlobalInterestCache(name):\n",
      "    try:\n",
      "        return __INTEREST_CACHE[name]\n",
      "    except:\n",
      "        return None\n",
      "\n",
      "def saveToGlobalInterestCache(name, value):\n",
      "    __INTEREST_CACHE[name] = value\n",
      "\n",
      "def syncCaches():\n",
      "    global __NAME_CACHE\n",
      "    global __INTEREST_CACHE\n",
      "    try:\n",
      "        nameCacheFile = pickle.load(open('NAME_CACHE.pkl','r'))\n",
      "        __NAME_CACHE = dict(__NAME_CACHE.items() + nameCacheFile.items())\n",
      "        \n",
      "        sizePre = len(nameCacheFile)\n",
      "        sizePost = len(__NAME_CACHE)\n",
      "        if sizePre!=sizePost:\n",
      "            print 'Cache grew by: ',(sizePost - sizePre)\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    try:\n",
      "        interestCacheFile = pickle.load(open('INTEREST_CACHE.pkl','r'))\n",
      "        __INTEREST_CACHE = dict(__INTEREST_CACHE.items() + interestCacheFile.items())\n",
      "        \n",
      "        sizePre = len(interestCacheFile)\n",
      "        sizePost = len(__INTEREST_CACHE)\n",
      "        if sizePre!=sizePost:\n",
      "            print 'Cache 2 grew by: ',(sizePost - sizePre)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    pickle.dump(__NAME_CACHE, open('NAME_CACHE.pkl', 'w'))\n",
      "    pickle.dump(__INTEREST_CACHE, open('INTEREST_CACHE.pkl', 'w'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getName(nym_xml):\n",
      "    forms = nym_xml.findAll('form', {'type': 'nym'})\n",
      "    if len(forms)==1:\n",
      "        return forms[0].getText()\n",
      "    else:\n",
      "        print '!!! getName(nym_xml) -- length = ',len(forms)\n",
      "        return None\n",
      "\n",
      "def getNymData(nym_xml):\n",
      "    '''\n",
      "    Returns:\n",
      "      freq    The frequency of term on any form it was observed\n",
      "      name    The normalized way in which the text has been observed\n",
      "      nstype  Namescape type of entity: person, location, etc.\n",
      "    '''\n",
      "    freqs = nym_xml.findAll('usg', { 'type': 'frequency' })\n",
      "    freq = np.array([ int(f.getText()) for f in freqs]).max()\n",
      "    name = getName(nym_xml)\n",
      "    nstype = nym_xml.get('ns:type')\n",
      "    \n",
      "    return freq, name, nstype\n",
      "\n",
      "def getLangTitle(sourceWiki, title, targetLang='en'):\n",
      "    page = sourceWiki.Pages[title]\n",
      "    page = page.resolve_redirect()\n",
      "\n",
      "    for lang,langTitle in page.langlinks():\n",
      "        if lang==targetLang:\n",
      "            return langTitle\n",
      "    return None\n",
      "\n",
      "def getAllTitleOptions(title):\n",
      "    words = title.split(' ')\n",
      "    cases = range(2) # [ 0 1 ] ==> 0 lower, 1 title case\n",
      "    nWords = len(words)\n",
      "    titleOpts = []\n",
      "    for x in itertools.product(cases, repeat=nWords):\n",
      "        title =  [ words[i].lower() if x[i]==0 else words[i].title() for i in range(nWords) ]\n",
      "        titleOpts.append(' '.join(title))\n",
      "    return titleOpts\n",
      "\n",
      "def findFirstTitleMatch(sourceWiki, origTitle, targetLang='en', debug=False):\n",
      "    cached = getFromGlobalNameCache(origTitle)\n",
      "#    if cached is not None:\n",
      "#        print 'Cached saved time!'\n",
      "#        return cached\n",
      "    \n",
      "    titles = getAllTitleOptions(origTitle)\n",
      "    for title in titles:\n",
      "        title = mwclient.page.Page.normalize_title(title)\n",
      "        candidate = getLangTitle(sourceWiki, title, targetLang=targetLang)\n",
      "        \n",
      "        if debug:\n",
      "            print title,'-->',candidate\n",
      "        \n",
      "        if candidate is not None:\n",
      "            saveToGlobalNameCache(origTitle, candidate)\n",
      "            return candidate\n",
      "    return None # No translation was found !\n",
      "\n",
      "def wikiCountryInterest(wiki, pageTitle):\n",
      "    cached = getFromGlobalInterestCache(pageTitle)\n",
      "#    if cached is not None:\n",
      "#        print 'Cached saved time 2!'\n",
      "#        return cached\n",
      "    try:\n",
      "        ips, usrs, nrevs = wq.getContributionsForPage(wiki, pageTitle)\n",
      "        knwRevs, conf, nIP, nUsr, nBot, nUnkn = btbtools.prepareData(ips, usrs, bots)\n",
      "        expEdits = wq.getTotalContributions()\n",
      "\n",
      "        cmpEdits = btbtools.compareEdits(expEdits, knwRevs)\n",
      "\n",
      "        nl_e, nl_o, nl_m = cmpEdits['NL']\n",
      "        ca_e, ca_o, ca_m = cmpEdits['CA']\n",
      "\n",
      "        saveToGlobalInterestCache(pageTitle, (nl_o, ca_o, conf))\n",
      "        return (nl_o, ca_o, conf)\n",
      "    except Exception as err:\n",
      "        print 'PATCH IT! Error for word \"'+pageTitle+'\":',err.__class__,' ->',err\n",
      "        return None,None,None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# karinaFile = '../data/corpus.karina.nerINL.2012-10-26/nl.ns.d.9789020417159.k.xml'\n",
      "karinaFile = '../data/corpus.karina.nerINL.2012-10-26/nl.ns.d.9789044301717.k.xml'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tei = open(karinaFile).read()\n",
      "teiSoup = BeautifulSoup.BeautifulSoup(tei)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nymList = teiSoup.findAll('nym')\n",
      "nymData = [getNymData(nym) for nym in nymList]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataEn = []\n",
      "\n",
      "for freq,name,nstype in nymData:\n",
      "    try:\n",
      "        match = findFirstTitleMatch(wikiNL, name, targetLang='en')\n",
      "        if match is not None:\n",
      "            dataEn.append((name,match,nstype,freq))\n",
      "    except Exception as err:\n",
      "        print 'Error for word \"'+name+'\":',err.__class__,' ->',err\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.DataFrame(dataEn, columns=['OrigWord','Word','Type','Count'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data['TEMP'] = data['Word'].apply(lambda x: wikiCountryInterest(wikiEN, x))\n",
      "data['NLO'] = data['TEMP'].apply(lambda x: x[0])\n",
      "data['CAO'] = data['TEMP'].apply(lambda x: x[1])\n",
      "data['Conf'] = data['TEMP'].apply(lambda x: x[2])\n",
      "del data['TEMP']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expEdits = wq.getTotalContributions()\n",
      "NLE = expEdits['NL']\n",
      "CAE = expEdits['CA']\n",
      "\n",
      "data['Interest-NL'] = data.apply(lambda x: btbtools.relativeInterest(NLE, x['NLO']), axis=1)\n",
      "data['Interest-CA'] = data.apply(lambda x: btbtools.relativeInterest(CAE, x['CAO']), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nItems = data['Count'].sum()\n",
      "data['Total-NL'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-NL'],axis=1)\n",
      "data['Total-CA'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-CA'],axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.to_pickle('data-karina.tmp.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def processKarinaFile(inFile_XML, outFile_PKL):\n",
      "    syncCaches()\n",
      "    tei = open(inFile_XML).read()\n",
      "    teiSoup = BeautifulSoup.BeautifulSoup(tei)\n",
      "\n",
      "    nymList = teiSoup.findAll('nym')\n",
      "    nymData = [getNymData(nym) for nym in nymList]\n",
      "\n",
      "    dataEn = []\n",
      "\n",
      "    for freq,name,nstype in nymData:\n",
      "        try:\n",
      "            match = findFirstTitleMatch(wikiNL, name, targetLang='en')\n",
      "            if match is not None:\n",
      "                dataEn.append((name,match,nstype,freq))\n",
      "            else:\n",
      "                print 'Warning: No English data for word: ',name\n",
      "        except Exception as err:\n",
      "            print 'Error for word \"'+name+'\":',err.__class__,' ->',err\n",
      "    syncCaches()\n",
      "        \n",
      "    data = pd.DataFrame(dataEn, columns=['OrigWord','Word','Type','Count'])\n",
      "\n",
      "    data['TEMP'] = data['Word'].apply(lambda x: wikiCountryInterest(wikiEN, x))\n",
      "    data['NLO'] = data['TEMP'].apply(lambda x: x[0])\n",
      "    data['CAO'] = data['TEMP'].apply(lambda x: x[1])\n",
      "    data['Conf'] = data['TEMP'].apply(lambda x: x[2])\n",
      "    del data['TEMP']\n",
      "\n",
      "    expEdits = wq.getTotalContributions()\n",
      "    NLE = expEdits['NL']\n",
      "    CAE = expEdits['CA']\n",
      "\n",
      "    data['Interest-NL'] = data.apply(lambda x: btbtools.relativeInterest(NLE, x['NLO']), axis=1)\n",
      "    data['Interest-CA'] = data.apply(lambda x: btbtools.relativeInterest(CAE, x['CAO']), axis=1)\n",
      "\n",
      "    nItems = data['Count'].sum()\n",
      "    data['Total-NL'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-NL'],axis=1)\n",
      "    data['Total-CA'] = data.apply(lambda x: 100 * x['Count'] / nItems * x['Conf'] * x['Interest-CA'],axis=1)\n",
      "\n",
      "    data.to_pickle(outFile_PKL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "import os.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inFiles = glob.glob('../data/corpus.karina.nerINL.2012-10-26/*.xml')\n",
      "outFiles = [ inFile.replace('.xml', '.pkl') for inFile in inFiles ]\n",
      "\n",
      "#inFiles = [\n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9022914186.s.xml', \n",
      "##    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021413353.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021413396.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431294.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404319.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9044604279.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9085420415.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9041410252.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029026561.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021412411.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404114.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.902340744x.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9024292646.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029561610.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.902230292X.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431251.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023404866.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029505249.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9053333029.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029518219.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9064811091.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023409272.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9057134829.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9021485060.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029554606.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9025426158.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029530421.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9074336825.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029098961.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9041409068.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.906291232X.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9029528893.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.905000802X.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9027420351.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023432592.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9023431189.s.xml', \n",
      "#    '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9025802109.s.xml'\n",
      "#]\n",
      "# outFiles = [ inFile.replace('.xml', '.pkl') for inFile in inFiles ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fin, fout in zip(inFiles, outFiles):\n",
      "    if not os.path.isfile(fout):\n",
      "        print 'Processing ',fin,'...'\n",
      "        processKarinaFile(fin, fout)\n",
      "    else:\n",
      "        print 'Skipping ',fin,'...'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# PATCH !\n",
      "First check for files with missing values..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "\n",
      "expEdits = wq.getTotalContributions()\n",
      "NLE = expEdits['NL']\n",
      "CAE = expEdits['CA']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def needsPatching(data):\n",
      "    dataMissing = data[data['NLO'].isnull()]\n",
      "    if len(dataMissing)==0:\n",
      "        return []\n",
      "    else:\n",
      "        return dataMissing['Word']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for pklFile in glob.glob('../data/corpus.karina.nerINL.2012-10-26/*.pkl'):\n",
      "for pklFile in glob.glob('../data/corpus.sanders.nerINL.2012-10-24/*.pkl'):\n",
      "    data = pickle.load(open(pklFile, 'r'))\n",
      "    toPatch = needsPatching(data)\n",
      "    if len(toPatch)>0:\n",
      "        print pklFile, toPatch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If there are any, fix those missing values\n",
      " - Load the file with missing value\n",
      " - Compute missing values\n",
      " - Replace computed values on data frame\n",
      " - Save data frame to original file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word = 'European Parliament'\n",
      "pklFile = '../data/corpus.sanders.nerINL.2012-10-24/nl.ns.d.9035130022.s.pkl'\n",
      "\n",
      "data = pickle.load(open(pklFile, 'r'))\n",
      "data[data['Word']==word]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OrigWord</th>\n",
        "      <th>Word</th>\n",
        "      <th>Type</th>\n",
        "      <th>Count</th>\n",
        "      <th>NLO</th>\n",
        "      <th>CAO</th>\n",
        "      <th>Conf</th>\n",
        "      <th>Interest-NL</th>\n",
        "      <th>Interest-CA</th>\n",
        "      <th>Total-NL</th>\n",
        "      <th>Total-CA</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>304</th>\n",
        "      <td> EUROPEES PARLEMENT</td>\n",
        "      <td> European Parliament</td>\n",
        "      <td> organisation</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.056988</td>\n",
        "      <td> 0.069199</td>\n",
        "      <td> 0.35179</td>\n",
        "      <td> 0.859619</td>\n",
        "      <td> 0.219647</td>\n",
        "      <td> 0.051693</td>\n",
        "      <td> 0.013208</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "               OrigWord                 Word          Type  Count       NLO  \\\n",
        "304  EUROPEES PARLEMENT  European Parliament  organisation      2  0.056988   \n",
        "\n",
        "          CAO     Conf  Interest-NL  Interest-CA  Total-NL  Total-CA  \n",
        "304  0.069199  0.35179     0.859619     0.219647  0.051693  0.013208  "
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nlo,cao,conf = wikiCountryInterest(wikiEN, word)\n",
      "\n",
      "idx = data[data['Word']==word].index\n",
      "assert len(idx)==1 # If more than 1 index, something is wrong...\n",
      "idx = idx[0]\n",
      "\n",
      "inl = btbtools.relativeInterest(NLE, nlo)\n",
      "ica = btbtools.relativeInterest(CAE, cao)\n",
      "nItems = data['Count'].sum()\n",
      "count = data['Count'][idx]\n",
      "\n",
      "tnl = 100 * count / nItems * conf * inl\n",
      "tca = 100 * count / nItems * conf * ica\n",
      "\n",
      "data.loc[idx,'NLO'] = nlo\n",
      "data.loc[idx,'CAO'] = cao\n",
      "data.loc[idx,'Conf'] = conf\n",
      "data.loc[idx,'Interest-NL'] = inl\n",
      "data.loc[idx,'Interest-CA'] = ica\n",
      "data.loc[idx,'Total-NL'] = tnl\n",
      "data.loc[idx,'Total-CA'] = tca\n",
      "\n",
      "data[data['Word']==word]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>OrigWord</th>\n",
        "      <th>Word</th>\n",
        "      <th>Type</th>\n",
        "      <th>Count</th>\n",
        "      <th>NLO</th>\n",
        "      <th>CAO</th>\n",
        "      <th>Conf</th>\n",
        "      <th>Interest-NL</th>\n",
        "      <th>Interest-CA</th>\n",
        "      <th>Total-NL</th>\n",
        "      <th>Total-CA</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>304</th>\n",
        "      <td> EUROPEES PARLEMENT</td>\n",
        "      <td> European Parliament</td>\n",
        "      <td> organisation</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.056988</td>\n",
        "      <td> 0.069199</td>\n",
        "      <td> 0.35179</td>\n",
        "      <td> 0.859619</td>\n",
        "      <td> 0.219647</td>\n",
        "      <td> 0.051693</td>\n",
        "      <td> 0.013208</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "               OrigWord                 Word          Type  Count       NLO  \\\n",
        "304  EUROPEES PARLEMENT  European Parliament  organisation      2  0.056988   \n",
        "\n",
        "          CAO     Conf  Interest-NL  Interest-CA  Total-NL  Total-CA  \n",
        "304  0.069199  0.35179     0.859619     0.219647  0.051693  0.013208  "
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(data,open(pklFile, 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}